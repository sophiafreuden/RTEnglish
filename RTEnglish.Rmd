---
title: "RTEnglish"
author: "Sophia Freuden"
date: "1/7/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

```{r}
library(tm)
library(SentimentAnalysis)
library(syuzhet)
library(SnowballC)
library(wordcloud)
library(RColorBrewer)
library(RCurl)
library(textclean)
library(lubridate)
library(gt)
library(tidyverse)
```

```{r}
col_types = cols(
  date = col_character(),
  title = col_character(),
  content = col_character(),
  URL = col_character()
)

# col_types = col_types

# Be sure to use correct data file for your chosen search term.
data <- read_csv("mcfaul.csv", col_types = col_types)

data <- data %>%
  mutate(date = dmy(date))

# view(data)

# Some of the cells in the content column may appear empty in view(). Print one row in
# console to check. I suspect they are just too big to appear in view(), but are other-
# wise present.
```

```{r}
Encoding(data$content) <- "latin1"

data$content <- replace_non_ascii(data$content)
```

```{r}
corpus <- SimpleCorpus(VectorSource(data$content))
# view(corpus)
```

```{r}
corpus <- tm_map(corpus, stripWhitespace)

corpus <- tm_map(corpus, content_transformer(tolower))

corpus <- tm_map(corpus, removeNumbers)

corpus <- tm_map(corpus, removePunctuation)

corpus <- tm_map(corpus, removeWords, stopwords("english"))
```

```{r}
nonstem.corpus <- corpus
corpus <- tm_map(corpus, stemDocument)
```

```{r}
DTM <- DocumentTermMatrix(corpus)
# view(DTM)
# inspect(DTM)
```

```{r}
nDTM <- DocumentTermMatrix(nonstem.corpus)
# view(nDTM)
```

```{r}
sums <- as.data.frame(colSums(as.matrix(nDTM)))
sums <- rownames_to_column(sums) 
colnames(sums) <- c("term", "count")
sums <- arrange(sums, desc(count))
head <- sums[1:75,]

sums2 <- as.data.frame(as.matrix(nDTM))
sums2d <- as.data.frame(as.matrix(DTM))
```

```{r}
sums2$ArtDate <- data$date
sums2d$ArtDate <- data$date
```

```{r}
# Summed per date. Change between sums2 and sums2d (stemmed) as needed.

# CHANGE BELOW:
# term in sum() below and
# 'Term' and 'RT Seach Term' to whatever term/search term your data is based on.

sums2 %>%
  # The group_by and summarise below sum the articles by date. Alt version c'd out below.
  group_by(ArtDate) %>% 
  summarise(Frequency = sum(civilsociety)) %>%
  ggplot(aes(x = ArtDate, y = Frequency)) +
  geom_point() +
  geom_smooth(method = 'loess') +
  labs(
    title = "Term Frequency Per Article Over Time",
    subtitle = "Term: 'civilsociety', RT Search Term: 'mcfaul'",
    x = "Date",
    y = "Frequency",
    # Comment/uncomment caption below as needed. Add/delete comma in line above, too.
    caption = "Stemmed."
  ) +
  scale_x_date(date_breaks = "1 year", date_labels = "%Y") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  ggsave("plot1.png", width = 10)
```

```{r}
# Non-summed per date.
# Folloew "Change Below" instructions above, but sum() term is y below.

# sums2 %>%
#   ggplot(aes(x = ArtDate, y = soros)) +
#   geom_point() +
#   geom_smooth(method = 'loess') +
#   labs(
#     title = "Term Frequency Per Article Time",
#     subtitle = "Term: 'soros', RT Search Term: 'soros russia'",
#     x = "Date",
#     y = "Frequency"
#   ) +
#   scale_x_date(date_breaks = "1 year", date_labels = "%Y") +
#   theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
#   ggsave("plot2.png", width = 10)
```

```{r}
wordcloud(words = head$term, freq = head$count, min.freq = 50,
  max.words=100, random.order=FALSE, rot.per=0.35, 
  colors=brewer.pal(8, "Dark2"))

# This must be saved manually (right click the image when it appears.)
```

```{r}
# The word cloud above uses the non-stemmed documents. The one below used the stemmed
# documents. For word clouds specifically, it may be better to use non-stemmed documents.
```

```{r}
sums3 <- as.data.frame(colSums(as.matrix(DTM)))
sums3 <- rownames_to_column(sums3) 
colnames(sums3) <- c("term", "count")
sums3 <- arrange(sums3, desc(count))
head3 <- sums3[1:75,]
```

```{r}
wordcloud(words = head3$term, freq = head3$count, min.freq = 50,
  max.words=100, random.order=FALSE, rot.per=0.35, 
  colors=brewer.pal(8, "Dark2"))
```

```{r}
sent <- analyzeSentiment(DTM, language = "english")
# view(sent)

sent <- sent[,1:4]

sent <- as.data.frame(sent)

# view(sent)

sum1 <- tibble(summary(sent$SentimentGI))

final <- bind_cols(data, sent)

# head(final)
```

```{r}
sum2 <- as.data.frame(t(sum1))

# CHANGE BELOW 'RT Seach Term' to whatever search term your data is based on.
sum2 %>%
  gt() %>%
  tab_header(
    title = "Sentiment Polarization Summary",
    subtitle = "RT Search Term: 'mcfaul'"
    )  %>% 
  # #tab_source_note(
  #   source_note = "RT Search Term was entered with quotation marks for accuracy."
  #   ) %>% 
  cols_label(
    V1 = "Min.",
    V2 = "1st Qu.",
    V3 = "Median",
    V4 = "Mean",
    V5 = "3rd Qu.",
    V6 = "Max."
    ) %>% 
  gtsave("table1.png", zoom = 2.5, expand = 10)
```

```{r}
sent2 <- get_nrc_sentiment(data$content, language = "english")

sent3 <- as.data.frame(colSums(sent2))

sent3 <- rownames_to_column(sent3)

colnames(sent3) <- c("emotion", "count")
```

```{r}
ggplot(sent3, aes(x = emotion, y = count, fill = emotion)) +
  geom_bar(stat = "identity") +
  theme_minimal() +
  theme(legend.position="none", panel.grid.major = element_blank()) +
  labs(title = "Emotion Analysis",
       x = "Emotion",
       y = "Total Count",
       subtitle = "RT Search Term: 'mcfaul'"
       # caption = "RT Search Term was entered with quotation marks for accuracy."
       ) +
  scale_fill_brewer(palette="Paired") +
  ggsave("emo1.png", width = 10)
```











